{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Day 3\n",
    "### Today is all about statistical analysis and visualization using libraries \n",
    "\n",
    "We begin by importing our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/becharah/GESISPython/main/Project/Climate_twitter.csv\"\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average number of retweets using pandas \n",
    "\n",
    "# using pandas\n",
    "\n",
    "\n",
    "# now using numpy\n",
    "\n",
    "\n",
    "# now using statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should probably try to interpret this. Get the medium and mode using the statistics library\n",
    "# save the values as median_retweet and mode_retweet\n",
    "\n",
    "\n",
    "# Try the same with the maximum and minimun values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the describe method to get a summary of the data column \n",
    "\n",
    "\n",
    "# what is the typical way to visualize the distribution of numirical data?\n",
    "# We can use a histogram to visualize the distribution of the retweets\n",
    "\n",
    "\n",
    "# Try the same with a boxplot\n",
    "\n",
    "\n",
    "# Which one do you think is more informative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the average len of the tweets in our data using apply method\n",
    "\n",
    "\n",
    "# visualize the distribution of the length of the tweets\n",
    "\n",
    "# now use a violin plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to compare our individual tweet to the average. Did this tweet get more or less likes than the average tweet?\n",
    "\n",
    "\n",
    "\n",
    "# Use the apply method to create a new column called likes_vs_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for average length of text of tweets \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of tweets that have more likes than the average\n",
    "\n",
    "\n",
    "# count the number of tweets that have more text than the average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move on to analyse the content of the tweets. \n",
    "\n",
    "# Let's make a list of words to count that are relevant to climate change\n",
    "\n",
    "\n",
    "# count the number of tweets that contain each words in the list above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add a column that sums the number of words in the list above\n",
    "\n",
    "\n",
    "# which word is the most common in the tweets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data science purposes, we don't like boolean, we like 0 and 1\n",
    "\n",
    "# Change the values of a column\n",
    "\n",
    "\n",
    "# What happened there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go ahead and complete this list of to-do tasks for the data \n",
    "\n",
    "# 1. Create a subset of the data for the verified users. Call it vf_data\n",
    "\n",
    "\n",
    "# 2. Check the number of rows and columns of vf_data\n",
    "\n",
    "\n",
    "# 3. Check the number of missing values in vf_data\n",
    "\n",
    "\n",
    "# 4. Get the average number of followers of the verified users. Now do the same for unverified users.\n",
    "#    Which is bigger?\n",
    "\n",
    "\n",
    "#Two steps\n",
    "\n",
    "# One step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we do this in one line of code? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it now with another variable and check retweets\n",
    "\n",
    "\n",
    "# We see a big outlier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who is Granite media? Subset\n",
    "\n",
    "\n",
    "# What did they write?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our data further. We will use matplotlib\n",
    "\n",
    "\n",
    "# Let's plot the number of followers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the number of followers for verified users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If groups you will now do the following exercise\n",
    "\n",
    "#data = pd.read_csv(url, parse_dates=['date'])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the date of the tweets\n",
    "\n",
    "\n",
    "# limit the x axis to months in 2020\n",
    "\n",
    "\n",
    "# Clean label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot polarity of the tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of polarity and subjectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can improve this plot by adding color to the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also add names to the y and x axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's your turn create three visualizations of your choice using the data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save them as png files and use them on your paper!\n",
    "\n",
    "we will continue tomorrow with the last stage of our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final part, let's check how to do hypothesis testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T statistic: 2.2894683580127317\n",
      "P-value: 0.04781622111056696\n",
      "Reject the null hypothesis; there is a significant difference between the sample mean and the hypothesized population mean.\n"
     ]
    }
   ],
   "source": [
    "# Finally let's run hypotesis tests on the data. We will use the scipy library\n",
    "\n",
    "# Hypothesis testing example: One-sample t-test\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Given student scores\n",
    "student_scores = np.array([72, 89, 65, 73, 79, 84, 63, 76, 85, 75])\n",
    "\n",
    "# Hypothesized population mean\n",
    "mu = 70\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(student_scores, mu)\n",
    "print(\"T statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Setting significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis; there is a significant difference between the sample mean and the hypothesized population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis; there is no significant difference between the sample mean and the hypothesized population mean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject the null hypothesis; there is a significant difference between the petal lengths of Iris setosa and Iris versicolor.\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis testing example: Two-sample t-test\n",
    "\n",
    "# Import the necessary libraries:\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the Iris dataset:\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Filter the dataset for the two species we want to compare:\n",
    "setosa = iris[iris['species'] == 'setosa']\n",
    "versicolor = iris[iris['species'] == 'versicolor']\n",
    "\n",
    "# Extract the petal lengths for each species:\n",
    "setosa_petal_lengths = setosa['petal_length']\n",
    "versicolor_petal_lengths = versicolor['petal_length']\n",
    "\n",
    "# Perform the t-test:\n",
    "t_stat, p_value = stats.ttest_ind(setosa_petal_lengths, versicolor_petal_lengths)\n",
    "\n",
    "# Interpret the results:\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis; there is a significant difference between the petal lengths of Iris setosa and Iris versicolor.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis; there is no significant difference between the petal lengths of Iris setosa and Iris versicolor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to reject the null hypothesis; there is no significant relationship between the variables.\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis testing example: Chi-square test\n",
    "\n",
    "# Import the necessary libraries:\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Create a contingency table:\n",
    "\n",
    "# Define the data:\n",
    "data = {'A': [20, 10, 30], 'B': [15, 15, 30], 'C': [25, 5, 30]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform the chi-square test:\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(df)\n",
    "\n",
    "# Interpret the results:\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis; there is a significant relationship between the variables.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis; there is no significant relationship between the variables.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
